<!DOCTYPE html><html><head><meta charset="UTF-8"><title> Interactive Dance Music I </title><meta name="viewport" content="width=device-width, user-scalable=no"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="shortcut icon" href="/favicon.00dce1d5.ico" type="image/x-icon"><link rel="stylesheet" href="/init.366dd74d.css"><link rel="stylesheet" href="/article.df3435a8.css"></head><body> <header id="top-container" role="navigation"> </header> <main id="main-container"> <article id="article-container"> <h1 id="article-title"> Interactive Dance Music I </h1> <h2 id="article-subtitle"> Personalised motion-sound mappings using motion sensing and Max for Live </h2> <time id="article-date"> 2020.11.02 </time> <section id="article-content-container"> <details><summary>Table of Contents</summary>
<p><div class="table-of-contents"><ul><li><a href="#objectives">Objectives</a></li><li><a href="#practical-work">Practical Work</a></li><li><a href="#limitations-of-direct-mapping">Limitations of Direct Mapping</a></li></ul></div></p>
</details>
<h3 id="objectives">Objectives</h3>
<p>Building on last yearâ€™s Max device, Blue Space, I would like to continue exploring technical solutions for electronic music performance. Blue Space provided an easy way to generate a granular backing track / effect for instrumentalists, with a simple set of grain controls. In this new project I would like to use a dancerâ€™s movements to generate music, extending the â€˜ease of useâ€™ theme from Blue Space. I find the relationship between music and dancer fascinating because it makes the dancer into an instrument without necessarily introducing to them to the mechanics of how the music is produced. This means the dance will be an exploration of how the body effects sound, and that the dancer may have to rethink their movements; disrupting established patterns from the previously one-way relationship between music and dance.</p>
<h3 id="practical-work">Practical Work</h3>
<p>The inspiration for this idea comes from the <a href="https://www.youtube.com/watch?v=YERtJ-5wlhM">V Motion Project</a> a commercial project from V Energy NZ. The video depicts a MIDI interface projected onto a wall, controlled by skeleton data recorded by a Kinect facing the dancer. This turns the dancer into a MIDI controller with parameters mapped to joint coordinates (x, y, z for hands, head, etc). I acquired a Kinect a week before term, and so immediately started on â€˜practice-led research,â€™ getting the data from <a href="https://ni-mate.com/">NI mate</a> (the only up-to-date Kinect driver for Mac) into MaxMSP to control Blue Space. I mapped the left and right hand X coordinates to sample positions in Blue Space allowing easy control for the grain window. I made this into a Max for Live patch allowing parameters to be easily mapped to midi or audio effects in Ableton to explore the V Motion Project style of applying the Kinect to existing projects.</p>
<p><img src="https://raw.githubusercontent.com/haelyons/Website-Content/master/IDM/KinectMapperV2.png" alt="KinectMapperV2"></p>
<h3 id="limitations-of-direct-mapping">Limitations of Direct Mapping</h3>
<p>Mapping the Kinect revealed the limitations of its use as a midi controller, parallel to the V Motion Project; the piece is pre-written so only certain motions correspond to it, requiring a set of movements â€“ a choreography. I explored this by testing the direct application of mappings to a dancer the weekend of the 24th October, with Marguerite Luna from lâ€™Ã‰cole Internationale de ThÃ©Ã¢tre Jacques Lecoq in Paris, whom I went to high school with. Having prepared a number of instruments to evaluate her use of the system, I discovered that direct control over the music using triggers for notes was unwieldy and difficult to incorporate into a natural choreography without significant preparation on a single piece, similar to the use that Chris Vik makes of the Kinect. As such, an Operator patch being modulated using Auto Filter and Ring Modulator frequency proved much more natural, reacting quickly and working with Margueriteâ€™s improvisatory style. Perhaps the most interesting test was the mapping of velocity to sample position in Granulator II â€“Â file position progressively incrementing based on current velocity, making Marguerite progressively dance through a sound, while affecting certain parameters. While this could add more physicality to DJ sets &amp; midi performance, it limits the freedom of the dancer and the music, not being an exploration of how the body effects sound, nor making the dancer into a true composer.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>Unfortunately, the amount of time needed for each test and the design of the instruments in tandem with the complications of working with a dancer in-remote due to COVID eventually led to the idea of recording dance videos and analysing the motion-capture data to correlate waveform features and generate real-time audio.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> This is the current focus of my research, and this post will be followed up with a breakdown of approaches â€“ Markov vs. Deep Learning, waveform vs. symbolic â€“ before Christmas.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="http://ismir2018.ircam.fr/doc/pdfs2/vogl.pdf">http://ismir2018.ircam.fr/doc/pdfs2/vogl.pdf</a> <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="http://vision.cs.unc.edu/home/publications/kinect_music.pdf">http://vision.cs.unc.edu/home/publications/kinect_music.pdf</a> <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://chrisvik.com/mocapmusic/">https://chrisvik.com/mocapmusic/</a> <a href="#fnref3" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section> </section> <section id="article-navigation"> <div class="article-navigation-item article-navigation-next"> <a href="/article/3.html"> <div class="article-navigation-arrow article-navigation-next">ï¼œ</div> <div class="article-navigation-content article-navigation-next"> <p class="article-navigation-title">Interactive Dance Music II</p> <p class="article-navigation-subtitle">Embodied exploration of personalised motion-sound relationships using motion sensing and interactive ML</p> </div> </a> </div> <div class="article-navigation-item article-navigation-prev"> <a href="/article/4.html"> <div class="article-navigation-arrow article-navigation-prev">ï¼ž</div> <div class="article-navigation-content article-navigation-prev"> <p class="article-navigation-title">Thoughts on Networked Modular Synthesis</p> <p class="article-navigation-subtitle">Routing control voltages over the internet via MaxMSP</p> </div> </a> </div> </section> <section id="article-list-button-container"> <a href="/articles.html"> <div id="article-list-button">ðŸ“š</div> </a> </section> </article> </main> <script defer src="/init.ddb7c0df.js"></script>
</body></html>